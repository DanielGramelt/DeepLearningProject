{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Workpackage: Data engineering\n",
    "\n",
    "Research Question:\n",
    "**\"How far can we simplify the input data to be still able to distinguish between Hand, Paper, and Scissors?\"**\n",
    "\n",
    "Research Answer:\n",
    "We can simplify input pictures through by converting them to greyscale and reducing the resolution. Both methods can be used without loosing much of the needed elements. Additionaly we can blur the images, to remove details and only get rough shapes and then use segmentation methods like otsu to get a binary image with the shape of the hand. On simple and clear input images, this can work so good, that with the calculation of histograms one could distinguish the gestures without maschine learning at all. The drawbacks are, that one relies heavily on the selection of the segmentation method and thus needs to be carefully chosen. Another problem shows the segmentation of more complex data. There the segmentation with basic methods have shown to be very incorrect and partwise not usefull at all. But this could also be due to the fact that the implemented otsu method is a global threshold segmentation method, which is not siuted for this usecase. If one has a good segmentation method for this use case, one could as also implemented cut the background out, so that the ML algorithm just has to distinguish between face and hand if the segmentation method lacks of that capability.\n",
    "**All in all the simplest robust image we were able to generate, which could optimize the training robustly is the blurred greyscale image, which has a reduced resolution. Further evaluation needs to be done if the blurring really benefits the training.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import os, random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def segment_otsu(img):\n",
    "    bins_num = 256\n",
    "\n",
    "\n",
    "    # Get the image histogram\n",
    "    hist, bin_edges = np.histogram(img, bins=bins_num)\n",
    "\n",
    "    # Get normalized histogram if it is required\n",
    "\n",
    "    hist = np.divide(hist.ravel(), hist.max())\n",
    "\n",
    "    # Calculate centers of bins\n",
    "    bin_mids = (bin_edges[:-1] + bin_edges[1:]) / 2.\n",
    "\n",
    "    # Iterate over all thresholds (indices) and get the probabilities w1(t), w2(t)\n",
    "    weight1 = np.cumsum(hist)\n",
    "    weight2 = np.cumsum(hist[::-1])[::-1]\n",
    "\n",
    "    # Get the class means mu0(t)\n",
    "    mean1 = np.cumsum(hist * bin_mids) / weight1\n",
    "    # Get the class means mu1(t)\n",
    "    mean2 = (np.cumsum((hist * bin_mids)[::-1]) / weight2[::-1])[::-1]\n",
    "\n",
    "    inter_class_variance = weight1[:-1] * weight2[1:] * (mean1[:-1] - mean2[1:]) ** 2\n",
    "\n",
    "    # Maximize the inter_class_variance function val\n",
    "    index_of_max_val = np.argmax(inter_class_variance)\n",
    "\n",
    "    threshold = bin_mids[:-1][index_of_max_val]\n",
    "    print(\"Otsu's algorithm implementation thresholding result: \", threshold)\n",
    "    result = np.zeros_like(img)\n",
    "    result[img > threshold] = 255\n",
    "    result[img <= threshold] = 0\n",
    "\n",
    "    return result\n",
    "\n",
    "def lower_resolution_image (image):\n",
    "    return image.resize((80,60))\n",
    "\n",
    "def blur_image(image):\n",
    "    return gaussian_filter(image, sigma=1.5)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    dir = \"Dataset/Rock_Paper_Sissors_Photos/train/scissors\"\n",
    "    file = random.choice(os.listdir(dir))\n",
    "    path = os.path.join(dir, file)\n",
    "    print(path)\n",
    "    original_image = Image.open(path)\n",
    "    greyscale_image = Image.open(path).convert('L')  # convert to grayscale\n",
    "\n",
    "    return original_image, greyscale_image\n",
    "\n",
    "def crop_image(original, image):\n",
    "    summation_row_array = np.apply_along_axis(sum,axis=0,arr=image)\n",
    "    summation_column_array = np.apply_along_axis(sum,axis=1,arr=image)\n",
    "    delete_indices = []\n",
    "    max_row_value = 0\n",
    "    for index in range(len(summation_row_array)):\n",
    "        if summation_row_array[index] == max_row_value:\n",
    "            delete_indices.append(index)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for index in range(len(summation_row_array)-1,-1,-1):\n",
    "        if summation_row_array[index] == max_row_value:\n",
    "            delete_indices.append(index)\n",
    "        else:\n",
    "            break\n",
    "    original = np.delete(original,delete_indices,axis=1)\n",
    "    image = np.delete(image,delete_indices,axis=1)\n",
    "\n",
    "    delete_indices = []\n",
    "    max_column_value = 0\n",
    "    for index in range(len(summation_column_array)):\n",
    "        if summation_column_array[index] == max_column_value:\n",
    "            delete_indices.append(index)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for index in range(len(summation_column_array)-1,-1,-1):\n",
    "        if summation_column_array[index] == max_column_value:\n",
    "            delete_indices.append(index)\n",
    "        else:\n",
    "            break\n",
    "    original = np.delete(original,delete_indices,axis=0)\n",
    "    image = np.delete(image,delete_indices,axis=0)\n",
    "\n",
    "    return original, image\n",
    "\n",
    "def delete_background(original, segmentation):\n",
    "    original[segmentation == 0] = 255\n",
    "    return original\n",
    "\n",
    "def visualize_greyscale(image, name):\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    ax.imshow(image, cmap=plt.cm.gray)\n",
    "    #ax.set_title(name)\n",
    "    ax.axis('off')\n",
    "\n",
    "def visualize_image(image,name):\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(name)\n",
    "    ax.axis('off')\n",
    "\n",
    "def create_histogram(image):\n",
    "    column_data = np.zeros(image.shape[0])\n",
    "    for column in range(image.shape[0]):\n",
    "        column_data[column] = np.sum(image[column])\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    ax.plot(column_data, range(len(column_data)))\n",
    "    ax.invert_yaxis()\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_title('y plot')\n",
    "    ax.axis('off')\n",
    "    row_data = np.zeros(image.shape[1])\n",
    "\n",
    "    for row in range(image.shape[1]):\n",
    "        row_data[row] = image.sum(axis=0)[row]\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    ax.plot(range(len(row_data)), row_data)\n",
    "    ax.set_title('x plot')\n",
    "    ax.axis('off')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def segment_simple_use_case():\n",
    "    original, greyscale = get_data()\n",
    "    low_resolution = lower_resolution_image(greyscale)\n",
    "    blurred = blur_image(low_resolution)\n",
    "    segmented = segment_otsu(blurred)\n",
    "    cropped_original, cropped = crop_image(low_resolution, segmented)\n",
    "    background_deletion = delete_background(cropped_original, cropped)\n",
    "    create_histogram(cropped)\n",
    "\n",
    "    visualize_image(original,\"Original image\")\n",
    "    visualize_greyscale(greyscale, \"Grayscaling\")\n",
    "    visualize_greyscale(low_resolution, \"Resolution reduction\")\n",
    "    visualize_greyscale(blurred, \"Blurring\")\n",
    "    visualize_greyscale(segmented, \"Segmentation\")\n",
    "    visualize_greyscale(cropped, \"Centring and zooming in\")\n",
    "    visualize_greyscale(background_deletion, \"original without background\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segment_simple_use_case()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "\n",
    "def readin_data(path_array, label):\n",
    "    images = []\n",
    "    for path in path_array:\n",
    "        for filename in glob.glob(path):\n",
    "            image = cv2.imread(filename)\n",
    "            im = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY),(80,60),interpolation = cv2.INTER_AREA)\n",
    "            images.append(im)\n",
    "    label_array = np.full(len(images),label)\n",
    "    return images,label_array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils as us\n",
    "\n",
    "lable_tensor = us.to_categorical([0, 1, 2], num_classes=3)\n",
    "\n",
    "def lookup_nominal(nominal):\n",
    "    str_labels = ('Rock','Paper','Scissors')\n",
    "    return str_labels[nominal]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('Dataset/structured_data')\n",
    "validation_dir = pathlib.Path('Dataset/validation_set')\n",
    "image_count = len(list(data_dir.glob('*/*')))\n",
    "print(\"Total images: \" +str(image_count))\n",
    "\n",
    "rock_images = list(data_dir.glob('rock/*'))\n",
    "paper_images = list(data_dir.glob('paper/*'))\n",
    "scissors_images = list(data_dir.glob('scissors/*'))\n",
    "\n",
    "PIL.Image.open(str(scissors_images[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 60\n",
    "img_width = 60\n",
    "\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    color_mode='grayscale',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    validation_split=0.9999,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    color_mode='grayscale',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\",\n",
    "                          input_shape=(img_height,\n",
    "                                       img_width,\n",
    "                                       1)),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[43mclass_names\u001B[49m)\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m Sequential([\n\u001B[1;32m      4\u001B[0m     data_augmentation,\n\u001B[1;32m      5\u001B[0m     layers\u001B[38;5;241m.\u001B[39mRescaling(\u001B[38;5;241m1.\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m255\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m     layers\u001B[38;5;241m.\u001B[39mDense(num_classes, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutputs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m ])\n\u001B[1;32m     18\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     19\u001B[0m               loss\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mSparseCategoricalCrossentropy(from_logits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m     20\u001B[0m               metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, name=\"outputs\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
